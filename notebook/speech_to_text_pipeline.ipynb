{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Building A Speech Recognition Data Pipeline\n",
    "tags:\n",
    "  - Text Analytics\n",
    "  - python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Pipeline]({{ site.url }}/images/jfk_moon2.png)\n",
    "\n",
    "Interested in learning how to build a simple speech-to-text data pipeline in a few lines of code? Want to learn how to generate a beautiful data visualization to get insights into a speech or text? This article is for you.\n",
    "\n",
    "I'll walk thru how to build a speech recognition data pipeline using python and the Google Clouds speech-to-text API. We'll touch on the basics of building a network graph from a text data source. And we’ll finish off by generating data visualizations using Gephi.\n",
    "\n",
    "By the end of this article you'll be able to generate neat graphs like the image below.\n",
    "![Data Pipeline]({{ site.url }}/images/speech_graph.png)\n",
    "\n",
    "# Backstory\n",
    "\n",
    "I attended a tech conference in 2019. After the conference, I was at the airport waiting for my flight back to SFO. I began wrangling my notes into something coherent I could share with my team back at the office.\n",
    "\n",
    "I started with trying to outline the main themes of the keynote presentation. I had my notes, screen shots, and an mp3 audio file of the keynote speech. I didn't want to listen to the entire keynote again, but my notes weren't great. So I started playing with speech-to-text API's to see if there was an easy way to transcribe the audio. After a few hours a had a simple speech-to-text analytics pipeline working.\n",
    "\n",
    "Hopefully this walkthru will save people some time if they've got a similar use case.\n",
    "\n",
    "\n",
    "# Data Pipeline Overview\n",
    "![Data Pipeline]({{ site.url }}/images/pipeline.png)\n",
    "\n",
    "Here are the high level steps we'll go thru to build out our data pipeline:\n",
    "+ **Acquire the data.** Extracting audio from a youtube video via the youtube API.\n",
    "+ **Audio processing.** Converting audio into wav format and converting to mono.\n",
    "+ **Speech-to-text transcription.** Using Google Cloud services, we'll transcribe the audio speech into text.\n",
    "+ **Data enrichment.** Perform sentence and word tokenization. Text analytics. Data transformations.\n",
    "+ **Graph model.** Ingesting data into a network data model. Performing network transformations and statistics.\n",
    "+ **Visualization.** Viewing and creating a data visualization of our network model in Gephi.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "If you want to follow the steps in this article, you’ll need to install the following software on your machine:\n",
    "+ [Jupyter Notebook](https://jupyter.org/)\n",
    "+ [Python 3.7.3](https://www.python.org/downloads/)\n",
    "+ [Gephi 0.9.2](https://gephi.org/users/download/)\n",
    "+ [Google Cloud](cloud.google.com/)\n",
    "+ [Anaconda](https://www.anaconda.com/)\n",
    "\n",
    "Anaconda isn’t required. But it makes it easier to setup Jupyter and manage your python environment. Anaconda will have you up in running in no-time.\n",
    "\n",
    "For reference, I tested this code on a Macbook Pro 2018 (i5) running MacOS Mojave 10.14.4.\n",
    "\n",
    "## Google Cloud\n",
    "\n",
    "The speech transcription steps use the Google Cloud Speech-to-Text API. To access the API, you'll need to [sign-up](cloud.google.com/) for a google cloud developer account. The storage and Speech-to-Text API both offer free tiers. Make sure to watch your file size to stay in the free tier limits. \n",
    "\n",
    "If you don't want to sign-up for a google cloud account, you can use the streaming audio method via the public API key. The Ultimate Guide To Speech Recognition With Python article has a good write up on how to do that. The streaming method will get you up and running quick. But that method will not allow you to transcribe longer audio files longer than 1 min. But it can work with some mild data hi-jinx. If cutting audio into smaller sections and looping thru it via the streaming API doesn't scare you go for it. That's how I began playing around.\n",
    "\n",
    "## Required Python Packages\n",
    "\n",
    "You'll need to have the following list of python modules installed on your machine.\n",
    "\n",
    "Pip installation of the packages should be straightforward.\n",
    "\n",
    "```\n",
    "pip install YOUR_PACKAGE_NAME\n",
    "```\n",
    "\n",
    "### Google Cloud API\n",
    "+ gcloud==0.18.3\n",
    "+ google-api-core==1.9.0\n",
    "+ google-api-python-client==1.7.8\n",
    "+ google-auth==1.6.3\n",
    "+ google-auth-httplib2==0.0.3\n",
    "+ google-cloud==0.34.0\n",
    "+ google-cloud-core==0.29.1\n",
    "+ google-cloud-speech==1.0.0\n",
    "+ google-cloud-storage==1.15.0\n",
    "+ google-resumable-media==0.3.2\n",
    "+ googleapis-common-protos==1.5.9\n",
    "\n",
    "### Data Processing\n",
    "+ networkx==2.3\n",
    "+ nltk==3.4.1\n",
    "+ numpy==1.16.2\n",
    "+ python-louvain==0.13\n",
    "+ pandas==0.24.2\n",
    "\n",
    "### Audio Manipulation\n",
    "+ pydub==0.23.1\n",
    "\n",
    "### Youtube Download API\n",
    "+ youtube-dl==2019.4.30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Audio\n",
    "The first step is to find an audio file that you wish to transcribe. For this article, we'll use the [JFK moon speech](https://www.youtube.com/watch?v=ouRbkBAOGEw). I choose this audio source for a few reasons:\n",
    "+ The text transcript is available and can be used to cross check the speech to text output for accuracy\n",
    "+ The audio quality is not 100%, and I wanted to see how this would impact the text to speech transcription\n",
    "+ The speech had enough length and complexity to generate an neat data visual\n",
    "+ It's an awesome speech!\n",
    "\n",
    "If you already have an audio file you want to use, skip this step. Otherwise, for demo purposes you can use the code below to extract the audio from a youtube video to use as your audio source. The code block below will extract the audio portion of the requested video, and download it to your local machine in mp3 format. \n",
    "\n",
    "Just replace the youtube URL with the video URL you wish to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] ouRbkBAOGEw: Downloading webpage\n",
      "[youtube] ouRbkBAOGEw: Downloading video info webpage\n",
      "[youtube] ouRbkBAOGEw: Downloading js player vflusCuE1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: unable to extract channel id; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Destination: JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.m4a\n",
      "[download] 100% of 16.18MiB in 00:0161MiB/s ETA 00:007\n",
      "[ffmpeg] Correcting container in \"JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.m4a\"\n",
      "[ffmpeg] Destination: JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.mp3\n",
      "Deleting original file JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.m4a (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import youtube_dl\n",
    "\n",
    "video_url='https://www.youtube.com/watch?v=ouRbkBAOGEw'\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Processing - Prep Work\n",
    "The next step is to prepare the audio file for text transcription. Our mp3 audio file needs to be converted into wav format and mono audio. We'll use the pydub module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='/Users/jhuck/Documents/text_to_speech/JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.wav'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "\n",
    "directory_path = '/Users/jhuck/Documents/text_to_speech'\n",
    "input_file_name = 'JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.mp3'\n",
    "output_file_name = 'JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.wav'\n",
    "\n",
    "#initialize our audio segment and set channels to mono\n",
    "audio = AudioSegment.from_file(os.path.join(directory_path, input_file_name), format = 'mp3')\n",
    "audio = audio.set_channels(1)\n",
    "\n",
    "#export audio to wav format\n",
    "audio.export(os.path.join(directory_path, output_file_name), format = 'wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Setup\n",
    "Before accessing the Google APIs we need to do a bit of setup work to prep our local environment.\n",
    "\n",
    "## Storage Bucket\n",
    "The Google Cloud Speech-to-Text API has two processing methods. For audio sources < 1 min in length, the streaming API can be used. For audio sources > 1 min length, the file needs to be loaded into a Google Cloud storage bucket before processing.\n",
    "\n",
    "Setting up [Google Cloud Storage](https://cloud.google.com/products/storage/) only takes a few minutes. You'll need to sign-up for a Google Cloud account if you don't already have one. If you don't already have a Storage bucket setup, create one now. Save the name of your storage bucket. You will need it to access the API and upload the audio file.\n",
    "\n",
    "![Data Pipeline]({{ site.url }}/images/console.png)\n",
    "\n",
    "## API Key\n",
    "You need to generate an API key to access the Google Cloud API. The can be done via the Google Cloud Console. After you generate the key, download the API key JSON file to your local machine.\n",
    "\n",
    "![Data Pipeline]({{ site.url }}/images/google_api.png)\n",
    "\n",
    "## Enable the Speech-to-Text API\n",
    "You'll need to enable the speech to text API. Navigate to the Speech-to-Text API service and toggle the enable service switch.\n",
    "\n",
    "## Local Env Setup\n",
    "Before accessing the Google Cloud API's, you need to setup the credentials on your local environment. Local shell variable GOOGLE_APPLICATION_CREDENTIALS needs to be set. This can be done using the python commands below or via the terminal CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.join(directory_path, 'My First Project-02d9aeddc2f8.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload File to Google Cloud\n",
    "Now we're ready to upload our audio file to Google Cloud using the Storage API. Alternatively you can load the file into the storage bucket using the [Google Cloud Console GUI](https://cloud.google.com/storage/docs/quickstart-console). Google has a lot of [sample code snippets](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/storage/cloud-client/snippets.py) available for accessing the storage API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/jhuck/Documents/text_to_speech/JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.wav uploaded to JFK - We choose to go to the Moon, full length-ouRbkBAOGEw.wav.\n"
     ]
    }
   ],
   "source": [
    "gc_storage_bucket_name = your_storage_bucket_name\n",
    "\n",
    "upload_blob(gc_storage_bucket_name ,os.path.join(directory_path, output_file_name) ,output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Google Console to confirm the file has been uploaded. \n",
    "![JFK Moon]({{ site.url }}/images/storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription\n",
    "At this point we're ready to invoke the Speech-to-Text API and transcribe our audio file. The function below, transcribe_gcs, takes the URI of the file that we loaded to Google Cloud Storage an input. The file URI should be in format gs://YOUR_BUCKET_NAME/YOUR_FILE_NAME.\n",
    "\n",
    "## Timeout\n",
    "I've hardcoded the timeout variable to 400 seconds. To productionalize this you'd want to setup a polling function to keep checking for results from the API call. If you get any errors due to timeout, try upping the timeout variable. For reference 400 seconds was long enough to transcribe 20 minutes of audio.\n",
    "\n",
    "## Punctuation\n",
    "I've set the flag for punctuation to true. The API will attempt to add punctuation such as periods, commas, etc . We'll use the punctuation during the text processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    from google.cloud import speech\n",
    "    from google.cloud.speech import enums\n",
    "    from google.cloud.speech import types\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    audio = types.RecognitionAudio(uri=gcs_uri)\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        #sample_rate_hertz=16000,\n",
    "        language_code='en-US',\n",
    "        enable_automatic_punctuation=True)\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "    return_list = []\n",
    "    print('Waiting for operation to complete...')\n",
    "    response = operation.result(timeout=400)\n",
    "\n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    for result in response.results:\n",
    "        #The first alternative is the most likely one for this portion.\n",
    "        print(u'Transcript: {}'.format(result.alternatives[0].transcript))\n",
    "        print('Confidence: {}'.format(result.alternatives[0].confidence))\n",
    "        return_list.append(result.alternatives[0].transcript)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_uri = 'gs://' + gc_storage_bucket + '/' + output_file_name\n",
    "\n",
    "output_text = transcribe_gcs(audio_file_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you get the transcription text back, save it to a local file. You'll notice that a confidence measure is provided for each text transcription. Overall, the Google API did a good job of transcribing the speech. But there are definitely mistakes comparing to the hand transcribed sources. This is probably due to the low quality audio source. The audio contains recording artifacts and noise that make transcribing the speech difficult. \n",
    "\n",
    "The confidence measure could be used to identify and filter out low quality results. For our purposes, we’ll keep all of the audio transcription text the API generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text_file_name = 'jfk_moon.txt'\n",
    "\n",
    "with open(os.path.join(directory_path, output_text_file_name), 'w') as f:\n",
    "    for line in output_text:\n",
    "        f.write(\"%s\\n\" % line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "We've got our audio file transcription. It's time to start processing the text. For this article, our end goal is to produce a network visualization of the text. To achieve this, we need to transform the text to fit a network model. Specifically, an [undirected weighted graph](http://courses.cs.vt.edu/~cs3114/Fall10/Notes/T22.WeightedGraphs.pdf).\n",
    "\n",
    "![Network Graph]({{ site.url }}/images/node_edge.png)\n",
    "\n",
    "For our text, each word will become a node (or vertex). The relationship between the words will be the edges. Our final data output will be an array mapping the nodes and edges, including the weight (frequency).\n",
    "\n",
    "![Network Graph Example]({{ site.url }}/images/node_edge_example.png)\n",
    "\n",
    "For most of the text processing steps we'll be leveraging the excellent natural language toolkit (nltk) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "The next steps are about splitting the text into smaller chunks. We need to end up with individual words to map our nodes and edges.\n",
    "\n",
    "![Text Token]({{ site.url }}/images/text_process.png)\n",
    "\n",
    "Since we asked the Speech-to-Text API to add punctuation, there should be some level of punctuation available. We'll search each transcribed string in our output text for periods. If a period is detected, we'll use that to split the string into smaller tokens.\n",
    "\n",
    "At the end of this step, we want a list that contains sentence like elements. For example, given the sentence \"The fox jumped over the fence. And so did the lazy dog\". We'd want our array to contain two values: \"The fox jumped over the fence\", \"And so did the lazy dog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#break out text into sentances\n",
    "sentence_list = []\n",
    "\n",
    "for line in output_text:\n",
    "    if '.' not in line:\n",
    "        sentence_list.append(line.strip())\n",
    "    else:\n",
    "        sent = sent_tokenize(line)\n",
    "        for x in sent:\n",
    "            sentence_list.append(x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse the output and sample the data to confirm that the text was tokenized correctly. The text transcription service will not add punctuation 100% correctly. So expect to see some incorrect sentence structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president Pizza vice president', 'governor', 'Congressman Thomas', 'Senator while in Congress Mandela Bell', 'sinus sting which gas at ladies and gentlemen, I appreciate you your president having made me an honorary visiting professor and I will assure you that my first light you will be a very brief.', \"I am delighted to be here and I'm particularly delighted to be here on this occasion.\", 'We meet at a college.', 'noted for knowledge', 'They said he noted for progress in a state noted for strength and we stand in need of all three.', 'We meet in an hour of change and challenge.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "The next step is to further break down our sentences into individual words (tokens). Also, we want to add attribution to our words to identify the word class (noun, verb, punctuation, etc). Then we're going to do a few data wrangling steps to cleanup and filter the data remove unwanted data points.\n",
    "\n",
    "\n",
    "### Stopwords\n",
    "To extract the major themes from the text, we want to eliminate as much noise in the data as possible. One way to reduce noise is to filter out common \"stopwords\" like : a, for, so, etc . For our analysis, stop words will not add value and will be removed.\n",
    "\n",
    "### Alphanumerics\n",
    "I've added in a step to filter out non-alphabetical (A-Z) characters. You can leave these in if you think your text has significant non-alpha elements of value.\n",
    "\n",
    "### Lemmatization\n",
    "There's also a step to perform text lemmatization to try and normalize the word set(ie. truck, trucks, truck's = truck). More info on lemmatization found [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html).\n",
    "\n",
    "### Nouns\n",
    "For this walk-thru, we're only going to use nouns to build our text network. Nouns will help identify the major themes and simplify our word set.\n",
    "\n",
    "### Sentence Structure\n",
    "For the later processing steps, we want to keep the sentence structure mapping. For example, we want to know if a word was in the first or second sentence. Our output from this step will be a 2D list in format: (sentence number, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "word_list = []\n",
    "word_exclude_list = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_types = ['NOUN']\n",
    "words_to_keep = ['united-states']\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "for line in sentence_list:\n",
    "    tokenized_text = nltk.word_tokenize(line)\n",
    "    \n",
    "    #tokenized text is array with format [word, word type]\n",
    "    tagged_and_tokenized_text = nltk.pos_tag(tokenized_text,tagset='universal') \n",
    "\n",
    "    for token in tagged_and_tokenized_text:\n",
    "        \n",
    "        #For my use case i remove all non-alpha characters and lowered the text for uniformity. Modify for your use case\n",
    "        token = [re.sub('[^a-zA-Z]+', '', token[0]).lower(),token[1]]\n",
    "        \n",
    "        if (token[1] in word_types and token[0] not in stop_words and len(token[0]) > 1) or token[0] in words_to_keep:\n",
    "            word_list.append([record_count,lemmatizer.lemmatize(token[0])])\n",
    "\n",
    "        else:\n",
    "            word_exclude_list.append([record_count,token])\n",
    "        \n",
    "    record_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'president'], [0, 'pizza'], [0, 'vice'], [0, 'president'], [1, 'governor'], [2, 'congressman'], [2, 'thomas'], [3, 'senator'], [3, 'congress'], [3, 'mandela']]\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undirected Graph\n",
    "Now that we've got our list of words, it's time to transform the data to fit our network model. Every element in our word list is going to serve as a node. The relationships between the nodes will form our edges.\n",
    "\n",
    "Each individual word will serve as a node in our network. The edges between the nodes will be the word relationships within each sentence. To create the edge relationship, take each sentence and the associated set of words for that sentence. Then generate an [unordered set without replacement](https://www.probabilitycourse.com/chapter2/2_1_3_unordered_without_replacement.php) for each word in the sentence. \n",
    "\n",
    "For example, given set=(a,b,c), the output set would be:\n",
    "+ 1.(a,b)\n",
    "+ 2.(a,c)\n",
    "+ 3.(b,c)\n",
    "\n",
    "Sentence example: \"The sky is blue\"\n",
    "+ 1.(the, sky)\n",
    "+ 2.(the, is)\n",
    "+ 3.(the, blue)\n",
    "+ 4.(sky, is)\n",
    "+ 5.(sky, blue)\n",
    "+ 6.(is, blue)\n",
    "\n",
    "We're going to use the itertools module to help with this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "node_edge_list = []\n",
    "node_list = []\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "for word in word_list:\n",
    "    sentence = ([row for row in word_list if record_count == row[0]])\n",
    "\n",
    "    temp_list = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        temp_list.append(token[1])\n",
    "\n",
    "    temp_cartesian_list = list(it.combinations(temp_list, 2))\n",
    "\n",
    "    for i in temp_cartesian_list: \n",
    "        node_edge_list.append(i)\n",
    "        \n",
    "    record_count += 1\n",
    "\n",
    "#Sort the list to make sure column 1 is always < column 2 regarding sort.\n",
    "#Example, we don't want to count [(1,2), (2,1)]. We want [(1,2), (1,2)]\n",
    "#This will impact the step when we aggregate the pairs to calculate the edge weights.\n",
    "node_edge_list_array = np.array(node_edge_list)\n",
    "node_edge_list_sorted_array = np.sort(node_edge_list_array, axis=1)\n",
    "node_edge_list = node_edge_list_sorted_array.tolist()\n",
    "\n",
    "temp_node_list = []\n",
    "\n",
    "for i in node_edge_list:\n",
    "    temp_node_list.append(i[0])\n",
    "\n",
    "node_list = list(set(temp_node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pizza', 'president'], ['president', 'vice'], ['president', 'president'], ['pizza', 'vice'], ['pizza', 'president'], ['president', 'vice'], ['congressman', 'thomas'], ['congress', 'senator'], ['mandela', 'senator'], ['bell', 'senator']]\n"
     ]
    }
   ],
   "source": [
    "print(node_edge_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Weights\n",
    "To calculate the edge weights, you count the number of times that a pair of words appears in the array. Then aggregate the data to get the distinct node/edge combinations. SQL equivalent: \n",
    "\n",
    "```sql\n",
    "SELECT node\n",
    "    , edge\n",
    "    , COUNT(*) \n",
    "FROM word_array \n",
    "GROUP BY node, edge\n",
    "```\n",
    "\n",
    "We're going to use pandas for this step. Aggregations are super easy using pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['accelerator', 'automobile', 1], ['accelerator', 'equivalent', 1], ['accelerator', 'floor', 1], ['accelerator', 'power', 1], ['accuracy', 'canaveral', 1], ['accuracy', 'cape', 1], ['accuracy', 'line', 1], ['accuracy', 'missile', 1], ['accuracy', 'shot', 1], ['accuracy', 'stadium', 1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "node_edge_dataframe = pd.DataFrame(node_edge_list,columns=['node', 'edge'])\n",
    "node_edge_dataframe_count = node_edge_dataframe.groupby(['node','edge']).size().reset_index(name='counts')\n",
    "\n",
    "output_edge_list_with_counts = node_edge_dataframe_count.values.tolist()\n",
    "\n",
    "print(output_edge_list_with_counts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Graph\n",
    "At this point we're ready to generate the network from our node/edge array. We're going to use the networkx module to generate the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 347\n",
      "Number of edges: 2706\n",
      "Average degree:  15.5965\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "undirected_weighted_graph = nx.Graph()\n",
    "\n",
    "for i in output_edge_list_with_counts:\n",
    "    undirected_weighted_graph.add_edge(i[0], i[1], weight = i[2])\n",
    "    \n",
    "print(nx.info(undirected_weighted_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Partitioning\n",
    "To discover clusters or subgraphs within our network, we want to perform some type of graph partitioning to determine the node communities. This is a powerful step that will provide insights into the relationships that exist in our network.\n",
    "\n",
    "For this article, I used the [Louvain best community detection](https://python-louvain.readthedocs.io/en/latest/) algorithm as it was straightforward to use and produced good results for my use case. After we execute the algo, we update our network with the partition results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "graph_partition = community.best_partition(undirected_weighted_graph)\n",
    "nx.set_node_attributes(undirected_weighted_graph, graph_partition, 'best_community')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "We're done wrangling data. Onto the visualization steps. Since we intend to use Gephi to visualize the data, we need export our dataset to disk in gexf file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = 'network_output.gexf' \n",
    "nx.write_gexf(undirected_weighted_graph, os.path.join(directory_path, output_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "We're ready to visualize our data. If you haven't downloaded and installed Gephi, do it now. \n",
    "\n",
    "## Import GEXF File\n",
    "After you start Gephi, import the .gexf file we generated in the previous step.\n",
    "\n",
    "![Network0]({{ site.url }}/images/network0.png)\n",
    "\n",
    "## Generate Preview\n",
    "Go to the preview menu and run generate preview. At this point, you should see an incoherent jumble of black lines and dots.\n",
    "\n",
    "![Network1]({{ site.url }}/images/network1.png)\n",
    "\n",
    "## Add Node Partitioning\n",
    "In the appearance menu, click on node, node size, then partition. Click the drop down and choose the best community attribute. This will modify the color of the nodes based on the best community partition we derived earlier.\n",
    "\n",
    "![Network2]({{ site.url }}/images/network2.png)\n",
    "\n",
    "## Force Atlas Layout\n",
    "In the layout tab choose layout force atlas. Leave the default configurations. Hit run. You can stop it after a few seconds. This will rearrange the nodes based on the force layout algorithm. Feel free to play with the layouts or layout settings.\n",
    "\n",
    "![Network4]({{ site.url }}/images/network4.png)\n",
    "\n",
    "## Change Preview Preset\n",
    "Time to add some style. In the preview menu, change the style from default to black background. Feel free to use the default if you choose. The black layout looks cool though.\n",
    "\n",
    "After your done, click preview again. You should now have a graph output similar to the image below.\n",
    "\n",
    "![Network5]({{ site.url }}/images/network5.png)\n",
    "\n",
    "At this point you've got a cool looking network visualization. You can start digging in to find insights about the text. Try playing with the various filters to reduce noise. For large texts, filtering out nodes based on their [degree](https://en.wikipedia.org/wiki/Degree_distribution) or centrality can help to reduce graph clutter. For networks with a lage number of nodes, filters are your friend.\n",
    "\n",
    "# Wrap Up\n",
    "This is the first time I've worked with Gephi and the Google Cloud speech-to-text API. Both were fun to use and it was a great learning exercise. I've run several texts and audio sources thru this process and gotten some cool results. Viewing text as a network diagram opens up new pathways for insights. It's a different experience viewing a text or speech as a network graph. Easy to visually identify key themes using modularity clustering.\n",
    "\n",
    "Speech driven apps like Alexa and Siri are continuing to grow in popularity. Demand for analytics pipelines involving audio as a data source will also ramp up. There's a ton of potential for innovation in the space. Especially with streaming functionality. I'm excited to keep playing around with these tools.\n",
    "\n",
    "For reference, here's the full data viz image from the JFK moon speech.\n",
    "\n",
    "![Network5]({{ site.url }}/images/jfk_moon_network2.png)\n",
    "\n",
    "You can access the code used in this post at my github repo located [here](https://github.com/spanishjack/speechtotext).\n",
    "\n",
    "## References\n",
    "Here's a list of reference material I found helpful to get up to speed on speech-to-text, network graphs, and gephi.\n",
    "+ [realpython.com/python-speech-recognition/](https://realpython.com/python-speech-recognition/)\n",
    "+ (https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)\n",
    "+ https://cloud.google.com/speech-to-text/docs/\n",
    "+ https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0\n",
    "+ https://www.analyticsvidhya.com/blog/2018/04/introduction-to-graph-theory-network-analysis-python-codes/\n",
    "+ https://harangdev.github.io/applied-data-science-with-python/applied-social-network-analysis-in-python/1/\n",
    "+ https://neo4j.com/docs/graph-algorithms/current/algorithms/louvain/\n",
    "\n",
    "Thanks for reading the post. Feel free to contact me with any feedback or questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
